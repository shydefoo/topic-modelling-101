{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import nltk\n",
    "import tempfile\n",
    "\n",
    "from gensim.utils import simple_preprocess \n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "import pickle\n",
    "from gensim.models import LdaMulticore\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pprint\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "home = os.path.expanduser('~')\n",
    "repo_path = 'Documents/repos/mystuff/lda101' #change to path of repo\n",
    "repo = os.path.join(home, repo_path) \n",
    "pickled_data = os.path.join(repo, 'pickled_data')\n",
    "models = os.path.join(repo,'models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "postfix = 'abcdefghijklmno'\n",
    "def _assemble_w2v_file(postfix):\n",
    "    processed_docs = tempfile.NamedTemporaryFile(mode='ab', delete=True)\n",
    "\n",
    "    for letter in postfix:\n",
    "        fname = os.path.join(pickled_data,'processed_docs.pkl.a'+letter)\n",
    "        with open(fname, 'rb') as infile:\n",
    "            processed_docs.write(infile.read())\n",
    "\n",
    "    return processed_docs\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    # convert document into list of lowercase tokens, filter based on token length\n",
    "    for token in simple_preprocess(text):\n",
    "        if token not in STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dentary agriculture instead a part of delibera...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>yuyao is county level city in the northeast of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>liu huaqing october january wa general of the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hui pan nationalism refers to the common ident...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>man carrying shovel shovel is tool for digging...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             article\n",
       "0  dentary agriculture instead a part of delibera...\n",
       "1  yuyao is county level city in the northeast of...\n",
       "2  liu huaqing october january wa general of the ...\n",
       "3  hui pan nationalism refers to the common ident...\n",
       "4  man carrying shovel shovel is tool for digging..."
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with _assemble_w2v_file(postfix) as processed_docs_file:\n",
    "    f = open(processed_docs_file.name,'rb')\n",
    "    processed_docs = pickle.load(f)\n",
    "    f.close()\n",
    "# load models and data\n",
    "with open(os.path.join(repo,'dictionary.pkl'), 'rb') as file:\n",
    "    dictionary = pickle.load(file)\n",
    "# with open(os.path.join(repo,'bow_corpus.pkl'), 'rb') as file:\n",
    "#     bow_corpus = pickle.load(file)\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs] # bow rep for each article\n",
    "lda_model = LdaMulticore.load(os.path.join(models, 'lda.model'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDAModelDriver:\n",
    "    def __init__(self, dictionary, bow_corpus, lda_model):\n",
    "        self.dictionary = dictionary\n",
    "        self.bow_corpus = bow_corpus\n",
    "        self.lda_model = lda_model\n",
    "    \n",
    "    def lemmatize_stemming(self, text):\n",
    "        '''\n",
    "        lemmatize text, without pos tag, lemmatizer treats every word as noun. pos='v' tells lemmatizer to treat \n",
    "        each word as verb.\n",
    "        '''\n",
    "        word = WordNetLemmatizer().lemmatize(text, pos='v')\n",
    "        return stemmer.stem(word)\n",
    "    \n",
    "    def preprocess(self, text):\n",
    "        result = []\n",
    "        # convert document into list of lowercase tokens, filter based on token length\n",
    "        for token in simple_preprocess(text):\n",
    "            if token not in STOPWORDS and len(token) > 3:\n",
    "                result.append(self.lemmatize_stemming(token))\n",
    "        return result\n",
    "\n",
    "    def model_topics(self, text):\n",
    "        bow_vector = self.dictionary.doc2bow(self.preprocess(text))\n",
    "        for index, score in sorted(self.lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "            print(\"Score: {}\\t Topic: {}\".format(score, self.lda_model.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.3781326413154602\t Topic: 0.009*\"india\" + 0.009*\"countri\" + 0.008*\"relat\" + 0.006*\"embassi\" + 0.005*\"govern\"\n",
      "Score: 0.3524431586265564\t Topic: 0.007*\"nuclear\" + 0.004*\"number\" + 0.004*\"nation\" + 0.004*\"text\" + 0.003*\"counti\"\n",
      "Score: 0.16132183372974396\t Topic: 0.011*\"parti\" + 0.008*\"govern\" + 0.008*\"elect\" + 0.006*\"polit\" + 0.005*\"minist\"\n",
      "Score: 0.07931715250015259\t Topic: 0.005*\"test\" + 0.004*\"club\" + 0.003*\"cricket\" + 0.003*\"play\" + 0.003*\"australia\"\n",
      "Score: 0.02503322809934616\t Topic: 0.010*\"aircraft\" + 0.008*\"forc\" + 0.007*\"ship\" + 0.006*\"oper\" + 0.004*\"navi\"\n"
     ]
    }
   ],
   "source": [
    "from nytimes_scraper import get_text\n",
    "lda_model_driver = LDAModelDriver(dictionary, bow_corpus, lda_model)\n",
    "url = 'https://www.nytimes.com/1998/05/12/world/india-sets-3-nuclear-blasts-defying-a-worldwide-ban-tests-bring-a-sharp-outcry.html'\n",
    "lda_model_driver.model_topics(get_text(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
